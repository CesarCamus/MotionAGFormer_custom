{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "inputs, origin_img, center, scale = PreProcess(frame, track_bboxs, cfg, num_peroson)\n",
    "\n",
    "            inputs = inputs[:, [2, 1, 0]]\n",
    "\n",
    "            #print(\"inputs shape :\", inputs.shape)\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                inputs = inputs.cuda()\n",
    "            output = pose_model(inputs)\n",
    "\n",
    "            # compute coordinate\n",
    "            preds, maxvals = get_final_preds(cfg, output.clone().cpu().numpy(), np.asarray(center), np.asarray(scale))\n",
    "\n",
    "        kpts = np.zeros((num_peroson, 17, 2), dtype=np.float32)\n",
    "        scores = np.zeros((num_peroson, 17), dtype=np.float32)\n",
    "        for i, kpt in enumerate(preds):\n",
    "            kpts[i] = kpt\n",
    "\n",
    "        for i, score in enumerate(maxvals):\n",
    "            scores[i] = score.squeeze()\n",
    "\n",
    "        kpts_result.append(kpts)\n",
    "        scores_result.append(scores)\n",
    "\n",
    "    #print(traj)\n",
    "    keypoints = np.array(kpts_result)\n",
    "    scores = np.array(scores_result)\n",
    "\n",
    "    keypoints = keypoints.transpose(1, 0, 2, 3)  # (T, M, N, 2) --> (M, T, N, 2)\n",
    "    scores = scores.transpose(1, 0, 2)  # (T, M, N) --> (M, T, N)\n",
    "\n",
    "    return keypoints, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def PreProcess(image, bboxs, cfg, num_pos=2):\n",
    "    if type(image) == str:\n",
    "        data_numpy = cv2.imread(image, cv2.IMREAD_COLOR | cv2.IMREAD_IGNORE_ORIENTATION)\n",
    "        # data_numpy = cv2.cvtColor(data_numpy, cv2.COLOR_BGR2RGB)\n",
    "    else:\n",
    "        data_numpy = image\n",
    "\n",
    "    inputs = []\n",
    "    centers = []\n",
    "    scales = []\n",
    "\n",
    "    for bbox in bboxs[:num_pos]:\n",
    "        c, s = box_to_center_scale(bbox, data_numpy.shape[0], data_numpy.shape[1])\n",
    "        centers.append(c)\n",
    "        scales.append(s)\n",
    "        r = 0\n",
    "\n",
    "        trans = get_affine_transform(c, s, r, cfg.MODEL.IMAGE_SIZE)\n",
    "        input = cv2.warpAffine(\n",
    "            data_numpy,\n",
    "            trans,\n",
    "            (int(cfg.MODEL.IMAGE_SIZE[0]), int(cfg.MODEL.IMAGE_SIZE[1])),\n",
    "            flags=cv2.INTER_LINEAR)\n",
    "\n",
    "        transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "        input = transform(input).unsqueeze(0)\n",
    "        inputs.append(input)\n",
    "\n",
    "    inputs = torch.cat(inputs)\n",
    "    return inputs, data_numpy, centers, scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def box_to_center_scale(box, model_image_width, model_image_height):\n",
    "    \"\"\"convert a box to center,scale information required for pose transformation\n",
    "    Parameters\n",
    "    ----------\n",
    "    box : (x1, y1, x2, y2)\n",
    "    model_image_width : int\n",
    "    model_image_height : int\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (numpy array, numpy array)\n",
    "        Two numpy arrays, coordinates for the center of the box and the scale of the box\n",
    "    \"\"\"\n",
    "    center = np.zeros((2), dtype=np.float32)\n",
    "    x1, y1, x2, y2 = box[:4]\n",
    "    box_width, box_height = x2 - x1, y2 - y1\n",
    "\n",
    "    center[0] = x1 + box_width * 0.5\n",
    "    center[1] = y1 + box_height * 0.5\n",
    "\n",
    "    aspect_ratio = model_image_width * 1.0 / model_image_height\n",
    "    pixel_std = 200\n",
    "\n",
    "    if box_width > aspect_ratio * box_height:\n",
    "        box_height = box_width * 1.0 / aspect_ratio\n",
    "    elif box_width < aspect_ratio * box_height:\n",
    "        box_width = box_height * aspect_ratio\n",
    "    scale = np.array(\n",
    "        [box_width * 1.0 / pixel_std, box_height * 1.0 / pixel_std],\n",
    "        dtype=np.float32)\n",
    "    if center[0] != -1:\n",
    "        scale = scale * 1.25\n",
    "\n",
    "    return center, scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def get_affine_transform(\n",
    "        center, scale, rot, output_size,\n",
    "        shift=np.array([0, 0], dtype=np.float32), inv=0\n",
    "):\n",
    "    if not isinstance(scale, np.ndarray) and not isinstance(scale, list):\n",
    "        print(scale)\n",
    "        scale = np.array([scale, scale])\n",
    "\n",
    "    scale_tmp = scale * 200.0\n",
    "    src_w = scale_tmp[0]\n",
    "    dst_w = output_size[0]\n",
    "    dst_h = output_size[1]\n",
    "\n",
    "    rot_rad = np.pi * rot / 180\n",
    "    src_dir = get_dir([0, src_w * -0.5], rot_rad)\n",
    "    dst_dir = np.array([0, dst_w * -0.5], np.float32)\n",
    "\n",
    "    src = np.zeros((3, 2), dtype=np.float32)\n",
    "    dst = np.zeros((3, 2), dtype=np.float32)\n",
    "    src[0, :] = center + scale_tmp * shift\n",
    "    src[1, :] = center + src_dir + scale_tmp * shift\n",
    "    dst[0, :] = [dst_w * 0.5, dst_h * 0.5]\n",
    "    dst[1, :] = np.array([dst_w * 0.5, dst_h * 0.5]) + dst_dir\n",
    "\n",
    "    src[2:, :] = get_3rd_point(src[0, :], src[1, :])\n",
    "    dst[2:, :] = get_3rd_point(dst[0, :], dst[1, :])\n",
    "\n",
    "    if inv:\n",
    "        trans = cv2.getAffineTransform(np.float32(dst), np.float32(src))\n",
    "    else:\n",
    "        trans = cv2.getAffineTransform(np.float32(src), np.float32(dst))\n",
    "\n",
    "    return trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def get_3rd_point(a, b):\n",
    "    direct = a - b\n",
    "    return b + np.array([-direct[1], direct[0]], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def get_final_preds(config, batch_heatmaps, center, scale):\n",
    "    coords, maxvals = get_max_preds(batch_heatmaps)\n",
    "\n",
    "    heatmap_height = batch_heatmaps.shape[2]\n",
    "    heatmap_width = batch_heatmaps.shape[3]\n",
    "\n",
    "    # post-processing\n",
    "    if config.TEST.POST_PROCESS:\n",
    "        for n in range(coords.shape[0]):\n",
    "            for p in range(coords.shape[1]):\n",
    "                hm = batch_heatmaps[n][p]\n",
    "                px = int(math.floor(coords[n][p][0] + 0.5))\n",
    "                py = int(math.floor(coords[n][p][1] + 0.5))\n",
    "                if 1 < px < heatmap_width-1 and 1 < py < heatmap_height-1:\n",
    "                    diff = np.array(\n",
    "                        [\n",
    "                            hm[py][px+1] - hm[py][px-1],\n",
    "                            hm[py+1][px]-hm[py-1][px]\n",
    "                        ]\n",
    "                    )\n",
    "                    coords[n][p] += np.sign(diff) * .25\n",
    "\n",
    "    preds = coords.copy()\n",
    "\n",
    "    # Transform back\n",
    "    for i in range(coords.shape[0]):\n",
    "        preds[i] = transform_preds(\n",
    "            coords[i], center[i], scale[i], [heatmap_width, heatmap_height]\n",
    "        )\n",
    "\n",
    "    return preds, maxvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def get_max_preds(batch_heatmaps):\n",
    "    '''\n",
    "    get predictions from score maps\n",
    "    heatmaps: numpy.ndarray([batch_size, num_joints, height, width])\n",
    "    '''\n",
    "    assert isinstance(batch_heatmaps, np.ndarray), \\\n",
    "        'batch_heatmaps should be numpy.ndarray'\n",
    "    assert batch_heatmaps.ndim == 4, 'batch_images should be 4-ndim'\n",
    "\n",
    "    batch_size = batch_heatmaps.shape[0]\n",
    "    num_joints = batch_heatmaps.shape[1]\n",
    "    width = batch_heatmaps.shape[3]\n",
    "    heatmaps_reshaped = batch_heatmaps.reshape((batch_size, num_joints, -1))\n",
    "    idx = np.argmax(heatmaps_reshaped, 2)\n",
    "    maxvals = np.amax(heatmaps_reshaped, 2)\n",
    "\n",
    "    maxvals = maxvals.reshape((batch_size, num_joints, 1))\n",
    "    idx = idx.reshape((batch_size, num_joints, 1))\n",
    "\n",
    "    preds = np.tile(idx, (1, 1, 2)).astype(np.float32)\n",
    "\n",
    "    preds[:, :, 0] = (preds[:, :, 0]) % width\n",
    "    preds[:, :, 1] = np.floor((preds[:, :, 1]) / width)\n",
    "\n",
    "    pred_mask = np.tile(np.greater(maxvals, 0.0), (1, 1, 2))\n",
    "    pred_mask = pred_mask.astype(np.float32)\n",
    "\n",
    "    preds *= pred_mask\n",
    "    return preds, maxvals"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
